# Intent Analysis Prompt - Complete View

## ğŸ¯ The Actual Prompt Sent to LLM

**File**: `src/app/crews/chat/crew.py` (lines 185-230)

---

### Full Prompt Template:

```
Analyze this user message and determine the user's primary intent:

User message: "{user_message}"

Recent conversation history:
{history_summary if history_summary else "No previous messages"}

Classify the intent as ONE of these TWO categories based on the semantic meaning:

**SEARCH**: The user wants to retrieve or query information
- Questions about stored data (memories, tasks, lists)
- Questions about past events or conversations  
- General knowledge questions
- Uses question words: What, When, Where, Who, How, Why, Do, Does, Did, Is, Are, Can, Could, Would
- Examples:
  Â· "What is in my shopping list?"
  Â· "What did I do last Tuesday?"
  Â· "What is my cat's name?"
  Â· "Who built the Sagrada Familia?"
  Â· "When is my doctor appointment?"
  Â· "Do I have any tasks for today?"

**ACTION**: The user wants to store, create, modify, communicate, or provide information (DEFAULT)
- Statements with new information or events
- Commands to store, modify, or delete data
- Social interactions (greetings, thanks, acknowledgments)
- Responses providing information
- Any message that is NOT clearly a query
- When in doubt, choose ACTION
- Examples:
  Â· "Add tomatoes to shopping list"
  Â· "Hoy fuimos a la oficina con Biel"
  Â· "Her name is Luna, she's 3 years old"
  Â· "Hello" / "Thanks!" / "Ok"
  Â· "Create a task to call the doctor"
  Â· "I'm feeling tired today"

Think about what the user *wants to accomplish*:
- If they're ASKING for information â†’ SEARCH
- If they're TELLING you something â†’ ACTION (default)

Output format (must start with this):
Primary Intent: [SEARCH/ACTION]
Reasoning: [Brief explanation of why you chose this intent]
```

---

## ğŸ“ Example: Actual Prompt Sent in Last Execution

### Input to LLM:

```
Analyze this user message and determine the user's primary intent:

User message: "Hola, puedes detallar en que me puedes ayudar?"

Recent conversation history:
assistant: Based on what I found, it appears that Jorge Sunyer has signed up for 
a padel league called "la liguita" which takes place indoors at Hopitalet. 
Additionally, he is also registered at the B1 level.

user: Jorge Sunyer estÃ¡ tambiÃ©n apuntado a una liga de padel que se llama "la 
liguita". Se juega indoor en Hopitalet. EstÃ¡ apuntado en el nivel B1.

assistant: According to what I found, diarrhea caused by antibiotics can be 
uncomfortable and concerning for both children and adults...

Classify the intent as ONE of these TWO categories based on the semantic meaning:

**SEARCH**: The user wants to retrieve or query information
- Questions about stored data (memories, tasks, lists)
- Questions about past events or conversations  
- General knowledge questions
- Uses question words: What, When, Where, Who, How, Why, Do, Does, Did, Is, Are, Can, Could, Would
- Examples:
  Â· "What is in my shopping list?"
  Â· "What did I do last Tuesday?"
  Â· "What is my cat's name?"
  Â· "Who built the Sagrada Familia?"
  Â· "When is my doctor appointment?"
  Â· "Do I have any tasks for today?"

**ACTION**: The user wants to store, create, modify, communicate, or provide information (DEFAULT)
- Statements with new information or events
- Commands to store, modify, or delete data
- Social interactions (greetings, thanks, acknowledgments)
- Responses providing information
- Any message that is NOT clearly a query
- When in doubt, choose ACTION
- Examples:
  Â· "Add tomatoes to shopping list"
  Â· "Hoy fuimos a la oficina con Biel"
  Â· "Her name is Luna, she's 3 years old"
  Â· "Hello" / "Thanks!" / "Ok"
  Â· "Create a task to call the doctor"
  Â· "I'm feeling tired today"

Think about what the user *wants to accomplish*:
- If they're ASKING for information â†’ SEARCH
- If they're TELLING you something â†’ ACTION (default)

Output format (must start with this):
Primary Intent: [SEARCH/ACTION]
Reasoning: [Brief explanation of why you chose this intent]
```

---

### LLM Response (minimax-m2:cloud):

```
Primary Intent: SEARCH
Reasoning: The user is asking a direct question "puedes detallar en que me 
puedes ayudar?" (can you detail what you can help me with?) which uses the 
interrogative "can" and is specifically requesting information about my 
capabilities and services. This is a query about stored information regarding 
my functionality, making it a SEARCH intent rather than an action request.
```

---

## ğŸ” Prompt Analysis

### Structure:
1. **User message** - The actual message to classify
2. **Conversation history** - Context from recent exchanges (STM)
3. **Binary classification** - SEARCH or ACTION (removed CHAT)
4. **Semantic descriptions** - What each intent means
5. **Examples** - 6 examples per intent (âš ï¸ potential keyword matching issue)
6. **Decision heuristic** - "ASKING â†’ SEARCH, TELLING â†’ ACTION"
7. **Output format** - Structured response requirement

---

## âš ï¸ Potential Issues (From Earlier Discussion)

### Issue #1: Contains Literal Examples
```python
- Examples:
  Â· "What is in my shopping list?"
  Â· "What did I do last Tuesday?"
  Â· "What is my cat's name?"
  [...]
```

**Risk**: LLM might match patterns instead of understanding semantic meaning
**Your concern**: "did you fall again the the examples for the prompts?"

### Issue #2: Keywords Mentioned (But Not Enforced)
```python
- Uses question words: What, When, Where, Who, How, Why, Do, Does, Did, Is, Are, Can, Could, Would
```

**Note**: This is guidance, not strict keyword matching (better than planner prompt)

---

## âœ… What Works Well

### 1. Clear Binary Choice
- Only 2 options (SEARCH or ACTION)
- No ambiguous CHAT category
- Clear default (ACTION)

### 2. Semantic Focus
```
"Think about what the user *wants to accomplish*:
- If they're ASKING for information â†’ SEARCH
- If they're TELLING you something â†’ ACTION (default)"
```

### 3. Context Included
- Recent conversation history provides context
- Helps with multi-turn conversations

### 4. Structured Output
```
Primary Intent: [SEARCH/ACTION]
Reasoning: [Brief explanation]
```
Easy to parse and understand decisions

### 5. Default Bias
```
- When in doubt, choose ACTION
```
Ensures information isn't lost

---

## ğŸ“Š Performance with minimax-m2:cloud

**From test results**:
- **Accuracy**: 100% (18/18 correct)
- **Time**: 14.70s average for classification
- **Bilingual**: Perfect Spanish + English handling
- **No keyword dependency**: Understood semantic meaning

### Test Cases Passed:
âœ… "What is in my shopping list?" â†’ SEARCH
âœ… "Hoy fuimos a la oficina con Biel" â†’ ACTION
âœ… "DÃ³nde dejÃ© las llaves?" â†’ SEARCH
âœ… "Hello" â†’ ACTION
âœ… "QuÃ© tengo que hacer maÃ±ana?" â†’ SEARCH

**No failures** - The examples didn't cause keyword matching issues with minimax-m2

---

## ğŸ”„ Comparison with Other Prompts

### Intent Analyzer (This Prompt):
- **Purpose**: Binary SEARCH/ACTION classification
- **Style**: Semantic descriptions with examples
- **Issues**: Contains 12 literal examples (but works well in practice)
- **Performance**: 100% accuracy with minimax-m2

### Action Planner (Problematic):
- **Purpose**: Classify as task.create, list.add, memory.note
- **Style**: HEAVY keyword matching
- **Issues**: "CRITICAL: If phrase contains 'a la lista' â†’ ALWAYS use list.*"
- **Performance**: Would fail on semantic variations

### Search Coordinator:
- **Purpose**: Determine which sources to search
- **Style**: Strategic analysis
- **Issues**: Keywords in backstory ("keywords like 'remember', 'task'")
- **Performance**: Good analysis but recommendations ignored

---

## ğŸ’¡ Optimization Possibilities

### Option 1: Keep As Is
**Reasoning**: 
- 100% accuracy in tests
- minimax-m2 handles it well
- Examples don't cause keyword matching
- "If it ain't broke, don't fix it"

### Option 2: Remove Examples
**Change**:
```diff
**SEARCH**: The user wants to retrieve or query information
- Questions about stored data (memories, tasks, lists)
- Questions about past events or conversations  
- General knowledge questions
- Uses question words: What, When, Where, Who, How, Why...
- Examples:                                                    â† REMOVE THIS
-  Â· "What is in my shopping list?"                            â† REMOVE THIS
-  Â· "What did I do last Tuesday?"                             â† REMOVE THIS
-  [... all examples ...]                                      â† REMOVE THIS
```

**Trade-off**: 
- âœ… More semantic, less keyword-dependent
- âŒ Might reduce accuracy slightly
- â“ Need to test impact

### Option 3: Make Examples More Abstract
**Change**:
```diff
- Examples:
-  Â· "What is in my shopping list?"
-  Â· "What did I do last Tuesday?"
+  Â· Questions about stored information
+  Â· Questions about past events
```

---

## ğŸ¯ Recommendation

**Keep the current prompt** because:
1. âœ… 100% accuracy with minimax-m2:cloud
2. âœ… Handles bilingual perfectly
3. âœ… 14.7s execution time is acceptable
4. âœ… Examples aren't causing keyword matching issues
5. âœ… Clear structure and output format

**Priority**: Fix the Action Planner prompt first (has CRITICAL keyword rules)

---

## ğŸ“‹ Summary

**Current Intent Analysis Prompt**:
- Binary SEARCH/ACTION classification
- Includes conversation history
- Has 12 literal examples (but works well)
- Uses semantic descriptions
- Emphasizes "ASKING vs TELLING" heuristic
- Defaults to ACTION when uncertain
- Achieves 100% accuracy with minimax-m2:cloud

**Time**: 14.70s per classification  
**Accuracy**: 100% (18/18 test cases)  
**Status**: âœ… Working well, low priority for changes
